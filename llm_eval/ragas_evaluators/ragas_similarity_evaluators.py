from ragas.metrics import NonLLMStringSimilarity, StringPresence, ExactMatch, SemanticSimilarity
from ragas.embeddings import LangchainEmbeddingsWrapper
from model_tools import get_ragas_wrapped_azure_open_ai_embedding_model
from llm_eval.ragas_evaluators.ragas_base_evaluator import RagasBaseEvaluator


class RunSemanticSimilarity(RagasBaseEvaluator):
    """
       Evaluation Class: Similarity
       Evaluation Method: Embedding/Cosine Similarity
       Granularity: Medium

       A wrapper class for evaluating the semantic similarity between a model-generated response
       and a predefined ground truth using sentence-level embeddings. Any embedding model can be used in a
       LangchainEmbeddingsWrapper

       The evaluator is useful for:
       - Assessing how well a model's response semantically matches a reference answer.
       - Benchmarking model performance on tasks like question-answering, summarization, and dialogue generation.

       Attributes:
           response (str): The response generated by the model.
           reference (str): The expected correct response (ground truth).
           threshold (float): The minimum similarity score between 0.0 and 1.0.
           embedding_model (LangchainEmbeddingsWrapper): Optional embedding model to calculate the similarity score.
               If not provided, a default Azure OpenAI embedding model will be used.
           ragas_metric_args (dict): Optional arguments for configuring the RAGAS metric.
    """

    def __init__(self, response: str, reference: str, threshold: float,
                 embedding_model: LangchainEmbeddingsWrapper = None):
        embedding_model = embedding_model or get_ragas_wrapped_azure_open_ai_embedding_model()
        super().__init__(response, reference, threshold, SemanticSimilarity, {"embeddings": embedding_model})


class RunNonLLMStringSimilarity(RagasBaseEvaluator): #TODO add distance measures param
    """
        Evaluation Class: RunNonLLMStringSimilarity
        Evaluation Method: String Distance
        Granularity: Low

        This class is used to run the Non-LLM string similarity evaluation, which calculates the string similarity
        between a model-generated response and a reference string by measuring string distances. The evaluation is based
        on distance measures like Levenshtein, Hamming, Jaro, or Jaro-Winkler.

        Attributes:
            response (str): The model-generated response to evaluate.
            reference (str): The reference string against which the model's response will be compared.
            threshold (float): The minimum score required for passing the evaluation, based on the distance measure.
            """
    def __init__(self, response: str, reference: str, threshold: float):
        super().__init__(response, reference, threshold, NonLLMStringSimilarity)


class RunStringPresence(RagasBaseEvaluator):
    """
        Evaluation Class: Similarity
        Evaluation Method: String
        Granularity: Low

        This class is used to evaluate whether a reference string is present within a model-generated response.
        The evaluation is binary — it checks if the entire reference string appears in the response, returning a score
        of 1.0 if the reference string is present, and 0.0 if it is not.

        Attributes:
            response (str): The model-generated response to evaluate.
            reference (str): The reference string to check for presence in the model's response.
            """
    def __init__(self, response: str, reference: str):
        super().__init__(response, reference, False, StringPresence)


class RunExactMatch(RagasBaseEvaluator):
    """
        Evaluation Class: Similarity
        Evaluation Method: Srting
        Granularity: Low

        This class evaluates whether a model-generated response exactly matches a reference string. The evaluation
        is binary — it checks if the entire response string is identical to the reference string, returning a score
        of 1.0 if they are an exact match and 0.0 if they are not.

        Attributes:
            response (str): The model-generated response to evaluate.
            reference (str): The reference string that the model's response is compared against.
            """
    def __init__(self, response: str, reference: str):
        super().__init__(response, reference, False, ExactMatch)
