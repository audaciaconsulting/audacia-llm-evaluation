from typing import Optional
from azure.ai.evaluation import (
    SimilarityEvaluator,
    AzureOpenAIModelConfiguration,
    F1ScoreEvaluator,
)
from llm_eval.model_tools import get_azure_ai_evaluation_model_config
import logging

from tests.utils import format_dict_log

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RunSimilarityEvaluator:
    """
        Evaluation Class: Similarity
        Evaluation Method: Embedding
        Granularity: Medium

        A wrapper class for evaluating the semantic similarity between a model-generated response
        and a predefined ground truth using sentence-level embeddings.

        This class leverages the `SimilarityEvaluator`, which utilizes Azure OpenAI models
        to calculate a similarity score ranging from 1 (least similar) to 5 (most similar). The evaluation
        helps determine how closely a response matches the expected output in terms of semantic meaning. The `query` provides context to ensure accurate comparison between `response` and `ground_truth`,
        as meaning can vary with the prompt.

        The evaluator is useful in scenarios such as:
        - Validating the quality of AI-generated responses against known correct answers.
        - Benchmarking models on text generation tasks (e.g., question-answering, summarization).

        Attributes:
            query (str): The input query or prompt.
            response (str): The response generated by the model.
            ground_truth (str): The expected correct response.
            threshold (float): Minimum similarity score (0.0–5.0 scale) required to pass.
            model_config (Optional[AzureOpenAIModelConfiguration]): Configuration for the embedding model.
                If not provided, a default Azure AI configuration will be used.
        """

    def __init__(
            self,
            query: str,
            response: str,
            ground_truth: str,
            threshold: float,
            model_config: Optional[AzureOpenAIModelConfiguration],
    ):
        self.query = query
        self.response = response
        self.ground_truth = ground_truth
        self.threshold = threshold
        self.model_config = model_config or get_azure_ai_evaluation_model_config()

        if not 0.0 <= threshold <= 5.0:
            raise ValueError(f"Threshold must be between 0 and 5. Got {threshold}.")

    def __call__(self) -> dict:
        evaluator = SimilarityEvaluator(model_config=self.model_config, threshold=self.threshold)
        return evaluator(
            query=self.query,
            response=self.response,
            ground_truth=self.ground_truth,
        )

    def evaluate(self):
        result = self()

        result.update(
            {
                "query": self.query,
                "response": self.response,
                "ground_truth": self.ground_truth
            }
        )

        logger.info(format_dict_log(dictionary=result))
        assert result['similarity_result'] == 'pass'


class RunF1ScoreEvaluator:
    """
    Evaluation Class: Similarity
    Evaluation Method: Token
    Granularity: Low

    The F1 score is a harmonic mean of precision and recall. It measures the accuracy of a generated response
    by comparing it to a reference ground truth string. Precision is the fraction of relevant tokens among the
    retrieved ones (i.e., words in the generated response that also appear in the ground truth), while recall
    is the fraction of relevant tokens that were successfully retrieved (i.e., words in the ground truth that
    are also found in the generated response).

    The F1 score ranges from 0.0 to 1.0, with 1.0 indicating a perfect match. This metric is especially useful
    in tasks where both completeness (recall) and correctness (precision) matter — such as question answering,
    summarization, and any natural language generation task that involves comparison to reference text.

    Attributes:
        response (str): The model-generated response to evaluate.
        ground_truth (str): The correct answer or reference string.
        threshold (float): The F1 score threshold to determine a pass/fail outcome. Must be between 0 and 1.
    """

    def __init__(
            self,
            response: str,
            ground_truth: str,
            threshold: float,
    ):
        self.response = response
        self.ground_truth = ground_truth
        self.threshold = threshold

        if not 0.0 <= threshold <= 1.0:
            raise ValueError(f"Threshold must be between 0 and 1. Got {threshold}.")

    async def __call__(self) -> dict:
        evaluator = F1ScoreEvaluator(threshold=self.threshold)
        result = await evaluator._do_eval({
            "response": self.response,
            "ground_truth": self.ground_truth,
        })
        return result

    async def evaluate(self):
        result = await self()

        result.update(
            {
                "response": self.response,
                "ground_truth": self.ground_truth,
            }
        )

        logger.info(format_dict_log(dictionary=result))
        assert result['f1_result'] == 'pass'
